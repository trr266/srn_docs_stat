{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code can be used to download and perform some basic analysis on the SRN documents available from their API. \n",
    "\n",
    "Foremost, we want to look at issues we encounter while downloading and on a document and company level respectivly. This includes:\n",
    "\n",
    "- summary statistics of download \n",
    "- classify filetype of files downloaded\n",
    "- issues while downloading (specific error codes are printed)\n",
    "- problems specific to filetype\n",
    "- distribution of years covered by downloads \n",
    "- max, min, median and average pages in .pdf files\n",
    "\n",
    "make sure to set \"fpath\" to your desired local directory. By default, this links to the repository's \"data\" folder.  \n",
    "\n",
    "Python Version used: 3.10.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment \n",
    " \n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import filetype\n",
    "from datetime import datetime\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "srn_api_url = \"https://api.sustainabilityreportingnavigator.com/api/\"\n",
    "\n",
    "fpath = \"./\" # data/ your download directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write function: \n",
    "\n",
    "write function to be able to download files from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to write from api\n",
    "\n",
    "def get_srn_companies():\n",
    "    \"\"\"\n",
    "    Returns a list of companies that are included in SRN Document Database.\n",
    "\n",
    "    Returns:\n",
    "        [list{dict}]: A list containg company level metadata\n",
    "    \"\"\"\n",
    "    response = requests.get(srn_api_url + \"companies\", allow_redirects=True)\n",
    "    return response.json()\n",
    "\n",
    "def get_srn_documents():\n",
    "    \"\"\"\n",
    "    Returns a list of documents that are included in SRN Document Database.\n",
    "\n",
    "    Returns:\n",
    "        [list{dict}]: A list containg document level metadata\n",
    "    \"\"\"\n",
    "    response = requests.get(srn_api_url + \"documents\", allow_redirects=True)\n",
    "    return response.json()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df with document and company information \n",
    "companies = pd.DataFrame(get_srn_companies())\n",
    "documents = pd.DataFrame(get_srn_documents())\n",
    "df = documents.merge(companies, left_on='company_id', right_on='id')\n",
    "#df = df[['country', 'type', 'year', 'company_id']]\n",
    "#df['type'] = df['type'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download function:\n",
    "\n",
    "- download all documents from API (filename = id)\n",
    "- print error IDs of failed downloads\n",
    "- only downloads files that are not already in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download all documents from api\n",
    "\n",
    "def download_document(id, fpath, timeout=60):\n",
    "    \"\"\"\n",
    "    Retreives all documents available from the SRN Document Database and \n",
    "    stores it at the provided file path.\n",
    "\n",
    "    Args:\n",
    "        id (str): The SRN document id.\n",
    "        fpath (str): A sting containt the file path where you want to\n",
    "            store the file.\n",
    "        timeout (int, optional): Sometimes, a download API call might\n",
    "            nlock because of a dying connection or because the data\n",
    "            is not available. If a timeout is reached, the according\n",
    "            API request will raise an exception and and continue download with the next file    . \n",
    "            Defaults to 60 seconds.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(fpath):\n",
    "            response = requests.get(\n",
    "                srn_api_url + f\"documents/{id}/download\",\n",
    "                timeout=timeout\n",
    "            )\n",
    "            with open(fpath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded document {id}\")\n",
    "        else:\n",
    "            print(f\"Document {id} already exists in the directory\")\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Download timed out for document {id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while downloading document {id}: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    companies = get_srn_companies()\n",
    "    documents = get_srn_documents()\n",
    "    for doc in documents:\n",
    "        filepath = 0\n",
    "        filepath = f\"{fpath}/{doc['id']}\"\n",
    "        download_document(doc['id'], filepath)\n",
    "        print(f\"Downloaded document {doc['id']}\")\n",
    "    print(\"done!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classify function (filetype): \n",
    "\n",
    "classifies the filetype of downloaded documents \n",
    "\n",
    "for option to add suffix to each local file name, see \"add correct suffix to file\" below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetype classifier function - see below to add correct suffix to the local files\n",
    "\n",
    "directory = str(fpath)\n",
    "\n",
    "def classify_file(file_path):\n",
    "    if not os.path.exists(file_path):  # Check if the file exists\n",
    "        return 'NA'\n",
    "    kind = filetype.guess(file_path)\n",
    "    if kind is None:                   # \"html\" for not recognized files\n",
    "        return 'html'\n",
    "    return kind.extension\n",
    "\n",
    "def main():\n",
    "    for file_name in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        file_extension = classify_file(file_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create \"main_df\" \n",
    "\n",
    "prints number of documents that failed to download\n",
    "\n",
    "adds success and filetype colums (success=1 means that this id downloaded successfully)\n",
    "\n",
    "to do: add industry sector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create main_df for consecutive analysis\n",
    "\n",
    "fpath = fpath # this should not be necessary if the download code was run previously\n",
    "doc_api = pd.DataFrame(get_srn_documents()) # Create Pandas Dataframe for documents Information from API\n",
    "doc_local = pd.DataFrame({'id': os.listdir(fpath)}) # Create Pandas Dataframe for downloaded documents\n",
    "# doc_local['id'] = doc_local['id'].str.replace(r'\\.pdf$|\\.html$', '', regex=True) # remove suffix to be able to compare\n",
    "\n",
    "main_df = pd.merge(doc_api, doc_local, on='id', how='left') # merge the two dfs based on the id column\n",
    "main_df['success'] = main_df['id'].isin(doc_local['id']).astype(int) # add \"success\" column to show download success\n",
    "main_df['filetype'] = main_df['id'].apply(lambda x: classify_file(os.path.join(directory, x)))\n",
    "# main_df['filetype'] = main_df['href'].apply(lambda x: get_filetype(x)) # add \"filetype\" column, to determine pdf or not \n",
    "print(len(main_df[main_df['success'] == 0]), \"out of\", len(doc_api), \"documents unable to download\") # number of docs that werent able to be downloaded\n",
    "\n",
    "# export main_df as .csv in working directory and add date to filename\n",
    "rpath = \"output/\"\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "output_filename = f\"main_df_{current_date}.csv\"\n",
    "output_filepath = os.path.join(rpath, output_filename)\n",
    "main_df.to_csv(output_filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create \"filetype_downloads\": \n",
    "\n",
    "relative frequencies of document filetypes in local directory\n",
    "\n",
    "note: we can not make any statement about the filetype of unavailable files, hence all unsuccessful downloads report filetype NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test filetype\n",
    "\n",
    "filetype_downloads = main_df.groupby('success')['filetype'].value_counts(normalize=True)\n",
    "\n",
    "\n",
    "print(filetype_downloads)\n",
    "\n",
    "# export as .csv in working directory and add date to filename\n",
    "rpath = \"output/\"\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "output_filename = f\"filetype_downloads_{current_date}.csv\"\n",
    "output_filepath = os.path.join(rpath, output_filename)\n",
    "filetype_downloads.to_csv(output_filepath, index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create \"failed_downloads\": \n",
    "\n",
    "table displays only unsuccessful downloads for further analysis \n",
    "\n",
    "info: most missing files derive from a handful of companies\n",
    "\n",
    "to do: \n",
    "\n",
    "-add column for according download error messages\n",
    "\n",
    "-test for \"sector\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table: failed downloads\n",
    "\n",
    "# Filter the DataFrame for rows where 'success' equals 0\n",
    "failed_downloads = main_df[main_df['success'] == 0].copy()\n",
    "# Reset the index of the filtered DataFrame\n",
    "failed_downloads.reset_index(drop=True, inplace=True)\n",
    "# Sort the DataFrame by the \"name\" column\n",
    "failed_downloads.sort_values(by='name', inplace=True)\n",
    "\n",
    "# Calculate the total length of the filtered table\n",
    "total_length = len(failed_downloads)\n",
    "# Calculate the number of unique 'company_id' values in the filtered table\n",
    "unique_company_ids = failed_downloads['company_id'].nunique()\n",
    "# Calculate the average length of the filtered table per unique 'company_id'\n",
    "average_length = total_length / unique_company_ids\n",
    "\n",
    "print(\"Unsuccessful downloads per company in this df:\", (average_length))\n",
    "print(\"unsuccessful downloads derive from\", failed_downloads['company_id'].nunique(), \"companies\")\n",
    "\n",
    "# export as .csv in working directory and add date to filename\n",
    "rpath = \"output/\"\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "output_filename = f\"failed_downloads_{current_date}.csv\"\n",
    "output_filepath = os.path.join(rpath, output_filename)\n",
    "failed_downloads.to_csv(output_filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create \"years_downloads\": \n",
    "\n",
    "range and frequency of years covered by local documents\n",
    "\n",
    "to do: add column with frequency of years in API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test year\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'success' is 1\n",
    "success_df = main_df[main_df['success'] == 1]\n",
    "\n",
    "# Calculate the year counts for the filtered DataFrame\n",
    "year_counts = success_df['year'].value_counts().sort_index()\n",
    "total_count = year_counts.sum()\n",
    "\n",
    "# Create a new DataFrame to store the results\n",
    "years_downloads = pd.DataFrame({'Year': year_counts.index, 'Frequency': year_counts.values})\n",
    "\n",
    "# Calculate and add the relative frequency column\n",
    "years_downloads['Relative Frequency'] = (years_downloads['Frequency'] / total_count).round(2)\n",
    "\n",
    "print(years_downloads)\n",
    "\n",
    "# export as .csv in working directory and add date to filename\n",
    "rpath = \"output/\"\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "output_filename = f\"years_downloads_{current_date}.csv\"\n",
    "output_filepath = os.path.join(rpath, output_filename)\n",
    "years_downloads.to_csv(output_filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# total size of download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total size of download\n",
    "\n",
    "def get_directory_size(fpath):\n",
    "    total_size = 0\n",
    "    for file in os.listdir(fpath):\n",
    "        file_path = os.path.join(fpath, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            total_size += os.path.getsize(file_path)\n",
    "    return total_size\n",
    "\n",
    "directory_path = fpath\n",
    "size_in_bytes = get_directory_size(directory_path)\n",
    "size_in_kb = size_in_bytes / 1000\n",
    "size_in_mb = size_in_kb / 1000\n",
    "size_in_gb = size_in_mb / 1000\n",
    "\n",
    "print(\"Directory Size:\")\n",
    "print(f\"Bytes: {size_in_bytes}\")\n",
    "print(f\"KB: {size_in_kb}\")\n",
    "print(f\"MB: {size_in_mb}\")\n",
    "print(f\"GB: {size_in_gb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add correct suffix to local files\n",
    "\n",
    "basically the classify function from above \n",
    "\n",
    "might run into issues with descriptive analysis when running this first, before creating statistics tables\n",
    "\n",
    "to do: adjust code so this doesn't happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify filetype of downloads and add according suffix\n",
    "\n",
    "directory = \"/Volumes/Data SSD/2024_Bachelor Arbeit Data/raw/17042024_srn_docs/pdf + Suffix\" #fpath\n",
    "\n",
    "def classify_file(file_path):\n",
    "    kind = filetype.guess(file_path)\n",
    "    if kind is None:\n",
    "        return 'html'\n",
    "    return kind.extension\n",
    "\n",
    "def main():\n",
    "    for file_name in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        file_extension = classify_file(file_path)\n",
    "        new_file_name = f\"{file_name}.{file_extension}\"\n",
    "        new_file_path = os.path.join(directory, new_file_name)\n",
    "        os.rename(file_path, new_file_path)\n",
    "        print(f\"Renamed {file_name} to {new_file_name}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg, Median, Min and Max Statistics for downloaded PDF Docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# List all files in the directory\n",
    "pdf_files = [os.path.join(fpath, file) for file in os.listdir(fpath)]\n",
    "\n",
    "# Initialize lists to store statistics\n",
    "num_pages_list = []\n",
    "\n",
    "# Iterate over PDF files and extract statistics\n",
    "for file in pdf_files:\n",
    "    try:\n",
    "        with open(file, 'rb') as f:\n",
    "            reader = PdfReader(f)\n",
    "            num_pages_list.append(len(reader.pages))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file '{file}': {e}\")\n",
    "\n",
    "# Convert the list to a pandas Series\n",
    "pdf_stats = pd.Series(num_pages_list)\n",
    "\n",
    "# Print the statistics\n",
    "print(\"Average number of pages:\", pdf_stats.mean())\n",
    "print(\"Median number of pages:\", pdf_stats.median())\n",
    "print(\"Minimum number of pages:\", pdf_stats.min())\n",
    "print(\"Maximum number of pages:\", pdf_stats.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add preprocessed corpus to main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (see file preprocess_text.py)\n",
    "\n",
    "# directory with preprocessed text files\n",
    "ppath = \"/Volumes/Data SSD/2024_Bachelor Arbeit Data/processed/17042022_srn_docs_preprocessed\"\n",
    "\n",
    "# Iterate over the .txt files in ppath directory\n",
    "for file_name in os.listdir(ppath):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        file_path = os.path.join(ppath, file_name)\n",
    "        file_id = file_name.split(\".\")[0]  # Extract the id from the file name\n",
    "        \n",
    "        # Read the contents of the file\n",
    "        with open(file_path, \"r\") as file:\n",
    "            contents = file.read()\n",
    "        \n",
    "        # Update the 'corpus_preprocessed' column in main_df\n",
    "        main_df.loc[main_df['id'] == file_id, 'corpus_preprocessed'] = contents\n",
    "\n",
    "\n",
    "# Drop rows with NaN values in the 'corpus' column\n",
    "# df.dropna(subset=['corpus_preprocessed'], inplace=True)\n",
    "\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detection of Files\n",
    "not fully integrated yet\n",
    "need to: adjust the location or df of files that should be analysed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "# Define a function to apply language detection \n",
    "def detect_language(text):\n",
    "    if len(str(text)) < 3:  # Adjust the minimum length threshold as needed\n",
    "        return np.nan  # Return NaN if the text is too short\n",
    "    else:\n",
    "        return detect(str(text))  # Apply language detection if the text meets the length threshold\n",
    "\n",
    "# Apply language detection function with length check and store results in a new column\n",
    "main_df['language'] = main_df['corpus_preprocessed'].apply(detect_language)\n",
    "\n",
    "# Print the language for each row\n",
    "print(main_df['language'])\n",
    "\n",
    "# Get language summary\n",
    "language_summary = main_df['language'].value_counts()\n",
    "print(language_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export main_df as .csv in working directory and add date to filename\n",
    "rpath = \"output/\"\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "output_filename = f\"main_df_{current_date}.csv\"\n",
    "output_filepath = os.path.join(rpath, output_filename)\n",
    "main_df.to_csv(output_filepath, index=False)\n",
    "\n",
    "main_df\n",
    "\n",
    "# export main_df as .feather in working directory and add date to filename\n",
    "rpath = \"output/\"\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "output_filename = f\"main_df_{current_date}.feather\"\n",
    "output_filepath = os.path.join(rpath, output_filename)\n",
    "main_df.to_feather(output_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# error messages\n",
    "\n",
    "this is currently not yet working\n",
    "\n",
    "to do: add xls write function while downloading to later analyse these errors in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse error messages while downloading \n",
    "\n",
    "epath = \"your/directory\"\n",
    "df = pd.read_excel(epath)\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
