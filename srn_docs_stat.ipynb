{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code can be used to download and perform some basic analysis on the SRN documents available from their API. \n",
    "\n",
    "Foremost, we want to look at issues we encounter while downloading and on a document and company level respectivly. This includes:\n",
    "\n",
    "- summary statistics of download \n",
    "- classify filetype of files downloaded\n",
    "- issues while downloading (specific error codes are printed)\n",
    "- problems specific to filetype\n",
    "- distribution of years covered by downloads \n",
    "\n",
    "make sure to set \"fpath\" to your desired local directory. By default, this links to the repository's \"data\" folder.  \n",
    "\n",
    "Python Version used: 3.10.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment \n",
    " \n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import filetype\n",
    "from datetime import datetime\n",
    "\n",
    "srn_api_url = \"https://api.sustainabilityreportingnavigator.com/api/\"\n",
    "\n",
    "fpath = \"data/\" # your download directory goes here and is referred to throughout the code. By default, it is set to the \"data\" folder in your cloned repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write function: \n",
    "\n",
    "write function to be able to download files from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to write from api\n",
    "\n",
    "def get_srn_companies():\n",
    "    \"\"\"\n",
    "    Returns a list of companies that are included in SRN Document Database.\n",
    "\n",
    "    Returns:\n",
    "        [list{dict}]: A list containg company level metadata\n",
    "    \"\"\"\n",
    "    response = requests.get(srn_api_url + \"companies\", allow_redirects=True)\n",
    "    return response.json()\n",
    "\n",
    "def get_srn_documents():\n",
    "    \"\"\"\n",
    "    Returns a list of documents that are included in SRN Document Database.\n",
    "\n",
    "    Returns:\n",
    "        [list{dict}]: A list containg document level metadata\n",
    "    \"\"\"\n",
    "    response = requests.get(srn_api_url + \"documents\", allow_redirects=True)\n",
    "    return response.json()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download function:\n",
    "\n",
    "- download all documents from API (filename = id)\n",
    "- print error IDs of failed downloads\n",
    "- only downloads files that are not already in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download all documents from api\n",
    "\n",
    "def download_document(id, fpath, timeout=60):\n",
    "    \"\"\"\n",
    "    Retreives all documents available from the SRN Document Database and \n",
    "    stores it at the provided file path.\n",
    "\n",
    "    Args:\n",
    "        id (str): The SRN document id.\n",
    "        fpath (str): A sting containt the file path where you want to\n",
    "            store the file.\n",
    "        timeout (int, optional): Sometimes, a download API call might\n",
    "            nlock because of a dying connection or because the data\n",
    "            is not available. If a timeout is reached, the according\n",
    "            API request will raise an exception and and continue download with the next file    . \n",
    "            Defaults to 60 seconds.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(fpath):\n",
    "            response = requests.get(\n",
    "                srn_api_url + f\"documents/{id}/download\",\n",
    "                timeout=timeout\n",
    "            )\n",
    "            with open(fpath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded document {id}\")\n",
    "        else:\n",
    "            print(f\"Document {id} already exists in the directory\")\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Download timed out for document {id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while downloading document {id}: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    companies = get_srn_companies()\n",
    "    documents = get_srn_documents()\n",
    "    for doc in documents:\n",
    "        filepath = 0\n",
    "        filepath = f\"{fpath}/{doc['id']}\"\n",
    "        download_document(doc['id'], filepath)\n",
    "        print(f\"Downloaded document {doc['id']}\")\n",
    "    print(\"done!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classify function (filetype): \n",
    "\n",
    "classifies the filetype of downloaded documents \n",
    "\n",
    "for option to add suffix to each local file name, see \"add correct suffix to file\" below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier function, to add correct suffix to the local files, see below\n",
    "\n",
    "directory = str(fpath)\n",
    "\n",
    "def classify_file(file_path):\n",
    "    if not os.path.exists(file_path):  # Check if the file exists\n",
    "        return 'NA'\n",
    "    kind = filetype.guess(file_path)\n",
    "    if kind is None:\n",
    "        return 'html'\n",
    "    return kind.extension\n",
    "\n",
    "def main():\n",
    "    for file_name in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        file_extension = classify_file(file_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create \"main_df\" \n",
    "\n",
    "prints number of documents that failed to download\n",
    "\n",
    "adds success and filetype colums\n",
    "\n",
    "success=1 means that this id downloaded successfully\n",
    "\n",
    "to do: add sector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create main_df for consecutive analysis\n",
    "\n",
    "fpath = fpath # this should not be necessary if the download code was run previously\n",
    "doc_api = pd.DataFrame(get_srn_documents()) # Create Pandas Dataframe for documents Information from API\n",
    "doc_local = pd.DataFrame({'id': os.listdir(fpath)}) # Create Pandas Dataframe for downloaded documents\n",
    "# doc_local['id'] = doc_local['id'].str.replace(r'\\.pdf$|\\.html$', '', regex=True) # remove suffix to be able to compare\n",
    "\n",
    "main_df = pd.merge(doc_api, doc_local, on='id', how='left') # merge the two dfs based on the id column\n",
    "main_df['success'] = main_df['id'].isin(doc_local['id']).astype(int) # add \"success\" column to show download success\n",
    "main_df['filetype'] = main_df['id'].apply(lambda x: classify_file(os.path.join(directory, x)))\n",
    "# main_df['filetype'] = main_df['href'].apply(lambda x: get_filetype(x)) # add \"filetype\" column, to determine pdf or not \n",
    "print(len(main_df[main_df['success'] == 0]), \"out of\", len(doc_api), \"documents unable to download\") # number of docs that werent able to be downloaded\n",
    "\n",
    "# export main_df as .csv in working directory and add date to filename\n",
    "rpath = \"output/\"\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "output_filename = f\"main_df_{current_date}.csv\"\n",
    "output_filepath = os.path.join(rpath, output_filename)\n",
    "main_df.to_csv(output_filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create \"filetype_downloads\": \n",
    "\n",
    "relative frequencies of document filetypes in local directory\n",
    "\n",
    "note: we can not make any statement about the filetype of unavailable files, hence all unsuccessful downloads report filetype NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test filetype\n",
    "\n",
    "filetype_downloads = main_df.groupby('success')['filetype'].value_counts(normalize=True)\n",
    "\n",
    "\n",
    "print(filetype_downloads)\n",
    "\n",
    "# export as .csv in working directory and add date to filename\n",
    "rpath = \"output/\"\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "output_filename = f\"filetype_downloads_{current_date}.csv\"\n",
    "output_filepath = os.path.join(rpath, output_filename)\n",
    "filetype_downloads.to_csv(output_filepath, index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create \"failed_downloads\": \n",
    "\n",
    "table displays only unsuccessful downloads for further analysis \n",
    "\n",
    "info: most missing files derive from a handful of companies\n",
    "\n",
    "to do: \n",
    "\n",
    "-add column for according download error messages\n",
    "\n",
    "-test for \"sector\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table: failed downloads\n",
    "\n",
    "# Filter the DataFrame for rows where 'success' equals 0\n",
    "failed_downloads = main_df[main_df['success'] == 0].copy()\n",
    "# Reset the index of the filtered DataFrame\n",
    "failed_downloads.reset_index(drop=True, inplace=True)\n",
    "# Sort the DataFrame by the \"name\" column\n",
    "failed_downloads.sort_values(by='name', inplace=True)\n",
    "\n",
    "# Calculate the total length of the filtered table\n",
    "total_length = len(failed_downloads)\n",
    "# Calculate the number of unique 'company_id' values in the filtered table\n",
    "unique_company_ids = failed_downloads['company_id'].nunique()\n",
    "# Calculate the average length of the filtered table per unique 'company_id'\n",
    "average_length = total_length / unique_company_ids\n",
    "\n",
    "print(\"Unsuccessful downloads per company in this df:\", (average_length))\n",
    "print(\"unsuccessful downloads derive from\", failed_downloads['company_id'].nunique(), \"companies\")\n",
    "\n",
    "# export as .csv in working directory and add date to filename\n",
    "rpath = \"output/\"\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "output_filename = f\"failed_downloads_{current_date}.csv\"\n",
    "output_filepath = os.path.join(rpath, output_filename)\n",
    "failed_downloads.to_csv(output_filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create \"years_downloads\": \n",
    "\n",
    "range and frequency of years covered by local documents\n",
    "\n",
    "to do: add column with frequency of years in API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test year\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'success' is 1\n",
    "success_df = main_df[main_df['success'] == 1]\n",
    "\n",
    "# Calculate the year counts for the filtered DataFrame\n",
    "year_counts = success_df['year'].value_counts().sort_index()\n",
    "total_count = year_counts.sum()\n",
    "\n",
    "# Create a new DataFrame to store the results\n",
    "years_downloads = pd.DataFrame({'Year': year_counts.index, 'Frequency': year_counts.values})\n",
    "\n",
    "# Calculate and add the relative frequency column\n",
    "years_downloads['Relative Frequency'] = (years_downloads['Frequency'] / total_count).round(2)\n",
    "\n",
    "print(years_downloads)\n",
    "\n",
    "# export as .csv in working directory and add date to filename\n",
    "rpath = \"output/\"\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "output_filename = f\"years_downloads_{current_date}.csv\"\n",
    "output_filepath = os.path.join(rpath, output_filename)\n",
    "years_downloads.to_csv(output_filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# total size of download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total size of download\n",
    "\n",
    "def get_directory_size(fpath):\n",
    "    total_size = 0\n",
    "    for file in os.listdir(fpath):\n",
    "        file_path = os.path.join(fpath, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            total_size += os.path.getsize(file_path)\n",
    "    return total_size\n",
    "\n",
    "directory_path = fpath\n",
    "size_in_bytes = get_directory_size(directory_path)\n",
    "size_in_kb = size_in_bytes / 1000\n",
    "size_in_mb = size_in_kb / 1000\n",
    "size_in_gb = size_in_mb / 1000\n",
    "\n",
    "print(\"Directory Size:\")\n",
    "print(f\"Bytes: {size_in_bytes}\")\n",
    "print(f\"KB: {size_in_kb}\")\n",
    "print(f\"MB: {size_in_mb}\")\n",
    "print(f\"GB: {size_in_gb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add correct suffix to local files\n",
    "\n",
    "basically the classify function from above \n",
    "\n",
    "might run into issues with descriptive analysis when running this first, before creating statistics tables\n",
    "\n",
    "to do: adjust code so this doesn't happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify filetype of downloads and add according suffix\n",
    "\n",
    "directory = fpath\n",
    "\n",
    "def classify_file(file_path):\n",
    "    kind = filetype.guess(file_path)\n",
    "    if kind is None:\n",
    "        return 'html'\n",
    "    return kind.extension\n",
    "\n",
    "def main():\n",
    "    for file_name in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        file_extension = classify_file(file_path)\n",
    "        new_file_name = f\"{file_name}.{file_extension}\"\n",
    "        new_file_path = os.path.join(directory, new_file_name)\n",
    "        os.rename(file_path, new_file_path)\n",
    "        print(f\"Renamed {file_name} to {new_file_name}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg, Median, Min and Max Statistics for downloaded PDF Docs\n",
    "\n",
    "not yet tested\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "\n",
    "# Provide the directory containing PDF files\n",
    "pdf_directory = 'path/to/pdf/files'\n",
    "\n",
    "# List all files in the directory\n",
    "pdf_files = [os.path.join(pdf_directory, file) for file in os.listdir(pdf_directory)]\n",
    "\n",
    "# Calculate PDF statistics\n",
    "pdf_stats = pd.Series([PyPDF2.PdfFileReader(open(file, 'rb')).numPages for file in pdf_files])\n",
    "\n",
    "# Print the statistics\n",
    "print(\"Average number of pages:\", pdf_stats.mean())\n",
    "print(\"Median number of pages:\", pdf_stats.median())\n",
    "print(\"Minimum number of pages:\", pdf_stats.min())\n",
    "print(\"Maximum number of pages:\", pdf_stats.max())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# error messages\n",
    "\n",
    "this is currently not yet working\n",
    "\n",
    "to do: add xls write function while downloading to later analyse these errors in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse error messages while downloading \n",
    "\n",
    "epath = \"/Users/emil/srn_script/srn_error message/srn_errors.xlsx\"\n",
    "df = pd.read_excel(epath)\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
